{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62988601-706e-4bd5-b617-630ecf47bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import scripts\n",
    "from functools import lru_cache\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4fd07-fd56-408f-b190-4f83ccd2f95c",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "First we load the data. The basic idea is to create dictionaries with features associated to the drugs and cell-lines. In principle, the splits and the data shouldn't be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19b7d3c4-e803-4ba4-ba37-91295eb04378",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize = None)\n",
    "def get_data(n_fold = 0, fp_radius = 2):\n",
    "    smile_dict = pd.read_csv(\"data/smiles.csv\", index_col=0)\n",
    "    fp = scripts.FingerprintFeaturizer(R = fp_radius)\n",
    "    drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])\n",
    "    driver_genes = pd.read_csv(\"data/driver_genes.csv\").loc[:, \"symbol\"].dropna()\n",
    "    rnaseq = pd.read_csv(\"data/rnaseq_normcount.csv\", index_col=0)\n",
    "    driver_columns = rnaseq.columns.isin(driver_genes)\n",
    "    filtered_rna = rnaseq.loc[:, driver_columns]\n",
    "    tensor_exp = torch.Tensor(filtered_rna.to_numpy())\n",
    "    cell_dict = {cell: tensor_exp[i] for i, cell in enumerate(filtered_rna.index.to_numpy())}\n",
    "    data = pd.read_csv(\"data/GDSC1.csv\", index_col=0)\n",
    "    # default, remove data where lines or drugs are missing:\n",
    "    data = data.query(\"SANGER_MODEL_ID in @cell_dict.keys() & DRUG_ID in @drug_dict.keys()\")\n",
    "    unique_cell_lines = data.loc[:, \"SANGER_MODEL_ID\"].unique()\n",
    "    np.random.seed(420) # for comparibility, don't change it!\n",
    "    np.random.shuffle(unique_cell_lines)\n",
    "    folds = np.array_split(unique_cell_lines, 10)\n",
    "    test_lines = folds[0]\n",
    "    train_idxs = list(range(10))\n",
    "    train_idxs.remove(n_fold)\n",
    "    np.random.seed(420)\n",
    "    validation_idx = np.random.choice(train_idxs)\n",
    "    train_idxs.remove(validation_idx)\n",
    "    train_lines = np.concatenate([folds[idx] for idx in train_idxs])\n",
    "    validation_lines = folds[validation_idx]\n",
    "    test_lines = folds[n_fold]\n",
    "    train_data = data.query(\"SANGER_MODEL_ID in @train_lines\")\n",
    "    validation_data = data.query(\"SANGER_MODEL_ID in @validation_lines\")\n",
    "    test_data = data.query(\"SANGER_MODEL_ID in @test_lines\")\n",
    "    return (scripts.OmicsDataset(cell_dict, drug_dict, train_data),\n",
    "    scripts.OmicsDataset(cell_dict, drug_dict, validation_data),\n",
    "    scripts.OmicsDataset(cell_dict, drug_dict, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cccda52-600d-4931-b7ca-5c06db7d2d9f",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "we declare the configuration, this is going to be model-specific and we get the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7c23383-21e3-4fdd-a6fa-4b181c451af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"features\" : {\"fp_radius\":2},\n",
    "          \"optimizer\": {\"batch_size\": 512,\n",
    "                        \"clip_norm\":10,\n",
    "                        \"learning_rate\":0.0001,\n",
    "                        \"stopping_patience\":15},\n",
    "          \"model\":{\"embed_dim\":256,\n",
    "                 \"hidden_dim\":1024,\n",
    "                 \"dropout\":0.4,\n",
    "                 \"n_layers\": 3,\n",
    "                 \"norm\": \"batchnorm\"},\n",
    "         \"env\": {\"fold\": 0,\n",
    "                \"device\":\"cuda:3\",\n",
    "                 \"max_epochs\": 100,\n",
    "                 \"search_hyperparameters\":True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f80c659-4416-4017-902f-4d0bea4ccfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset, test_dataset = get_data(n_fold = config[\"env\"][\"fold\"],\n",
    "                                                           fp_radius = config[\"features\"][\"fp_radius\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eca57c-2e0a-42e6-8c9c-38b2ea9cb19c",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization\n",
    "\n",
    "we wrap the function for training the model in a function that can be used by optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58f25cf7-b977-48da-b569-8d231a6ac8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_optuna(trial, config):\n",
    "    def pruning_callback(epoch, train_r):\n",
    "        trial.report(train_r, step = epoch)\n",
    "        if np.isnan(train_r):\n",
    "            raise optuna.TrialPruned()\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    config[\"model\"] = {\"embed_dim\": trial.suggest_int(\"embed_dim\", 64, 512),\n",
    "                    \"hidden_dim\": trial.suggest_int(\"hidden_dim\", 64, 2048),\n",
    "                    \"n_layers\": trial.suggest_int(\"n_layers\", 1, 6),\n",
    "                    \"norm\": trial.suggest_categorical(\"norm\", [\"batchnorm\", \"layernorm\", None]),\n",
    "                    \"dropout\": trial.suggest_float(\"dropout\", 0.0, 0.5)}\n",
    "    config[\"optimizer\"] = { \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-1, log=True),\n",
    "                            \"clip_norm\": trial.suggest_int(\"clip_norm\", 0.1, 20),\n",
    "                            \"batch_size\": trial.suggest_int(\"batch_size\", 128, 512),\n",
    "                            \"stopping_patience\":10}\n",
    "    try:\n",
    "        R, model = scripts.train_model(config,\n",
    "                                       train_dataset,\n",
    "                                       validation_dataset,\n",
    "                                       use_momentum=True,\n",
    "                                       callback_epoch = pruning_callback)\n",
    "        return R\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4fad963-1f99-4aa3-8f1e-f5c614ef3bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2024-09-25 16:41:45,346]\u001b[0m A new study created in RDB with name: baseline_model\u001b[0m\n",
      "/home/alonsocampana/.local/lib/python3.9/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "/home/alonsocampana/.local/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (GroupwiseMetric). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_full_state_property`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/alonsocampana/train_fp_students/scripts/models.py:55: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3614.)\n",
      "  return torch.linalg.solve(A, Xy).T\n",
      "\u001b[32m[I 2024-09-25 16:45:50,346]\u001b[0m Trial 0 finished with value: 0.2619563501358032 and parameters: {'embed_dim': 423, 'hidden_dim': 1588, 'n_layers': 6, 'norm': None, 'dropout': 0.49271759139677374, 'learning_rate': 7.247880741514805e-05, 'clip_norm': 16, 'batch_size': 399}. Best is trial 0 with value: 0.2619563501358032.\u001b[0m\n",
      "\u001b[32m[I 2024-09-25 16:48:58,646]\u001b[0m Trial 1 finished with value: 0.19794464393258096 and parameters: {'embed_dim': 302, 'hidden_dim': 479, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.2622345933292263, 'learning_rate': 1.9277949196254086e-05, 'clip_norm': 10, 'batch_size': 367}. Best is trial 0 with value: 0.2619563501358032.\u001b[0m\n",
      "\u001b[32m[I 2024-09-25 16:52:37,558]\u001b[0m Trial 2 finished with value: 0.2861260999917984 and parameters: {'embed_dim': 357, 'hidden_dim': 946, 'n_layers': 6, 'norm': None, 'dropout': 0.276003638980854, 'learning_rate': 0.0006115366533057336, 'clip_norm': 19, 'batch_size': 451}. Best is trial 2 with value: 0.2861260999917984.\u001b[0m\n",
      "\u001b[32m[I 2024-09-25 16:55:56,796]\u001b[0m Trial 3 finished with value: 0.2696112312793732 and parameters: {'embed_dim': 141, 'hidden_dim': 1306, 'n_layers': 5, 'norm': None, 'dropout': 0.30653155903211937, 'learning_rate': 0.000137821216024339, 'clip_norm': 14, 'batch_size': 434}. Best is trial 2 with value: 0.2861260999917984.\u001b[0m\n",
      "\u001b[32m[I 2024-09-25 16:59:32,420]\u001b[0m Trial 4 finished with value: 0.1959408040761948 and parameters: {'embed_dim': 226, 'hidden_dim': 1596, 'n_layers': 4, 'norm': 'layernorm', 'dropout': 0.23946884911053173, 'learning_rate': 3.034390999775479e-05, 'clip_norm': 7, 'batch_size': 302}. Best is trial 2 with value: 0.2861260999917984.\u001b[0m\n",
      "\u001b[32m[I 2024-09-25 17:03:31,451]\u001b[0m Trial 5 finished with value: 0.22787449885606767 and parameters: {'embed_dim': 400, 'hidden_dim': 1329, 'n_layers': 6, 'norm': 'layernorm', 'dropout': 0.3245495217873006, 'learning_rate': 0.010982421845737302, 'clip_norm': 1, 'batch_size': 424}. Best is trial 2 with value: 0.2861260999917984.\u001b[0m\n",
      "\u001b[32m[I 2024-09-25 17:07:02,204]\u001b[0m Trial 6 finished with value: 0.2425125753641129 and parameters: {'embed_dim': 454, 'hidden_dim': 262, 'n_layers': 5, 'norm': None, 'dropout': 0.45585799344526423, 'learning_rate': 0.0008321435744973764, 'clip_norm': 15, 'batch_size': 202}. Best is trial 2 with value: 0.2861260999917984.\u001b[0m\n",
      "\u001b[33m[W 2024-09-25 17:08:09,705]\u001b[0m Trial 7 failed with parameters: {'embed_dim': 426, 'hidden_dim': 1268, 'n_layers': 5, 'norm': 'batchnorm', 'dropout': 0.03486968411459995, 'learning_rate': 0.016015558181210757, 'clip_norm': 6, 'batch_size': 474} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alonsocampana/.local/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_28830/856521.py\", line 10, in <lambda>\n",
      "    objective = lambda x: train_model_optuna(x, config)\n",
      "  File \"/tmp/ipykernel_28830/1795729836.py\", line 18, in train_model_optuna\n",
      "    R, model = scripts.train_model(config,\n",
      "  File \"/home/alonsocampana/train_fp_students/scripts/models.py\", line 230, in train_model\n",
      "    train_loss = train_step(model, optimizer, train_loader, config, device)\n",
      "  File \"/home/alonsocampana/train_fp_students/scripts/models.py\", line 192, in train_step\n",
      "    for x in loader:\n",
      "  File \"/home/alonsocampana/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/alonsocampana/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 674, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/alonsocampana/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/home/alonsocampana/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/home/alonsocampana/train_fp_students/scripts/utils.py\", line 17, in __getitem__\n",
      "    instance = self.data.iloc[idx]\n",
      "  File \"/home/alonsocampana/.local/lib/python3.9/site-packages/pandas/core/indexing.py\", line 1073, in __getitem__\n",
      "    return self._getitem_axis(maybe_callable, axis=axis)\n",
      "  File \"/home/alonsocampana/.local/lib/python3.9/site-packages/pandas/core/indexing.py\", line 1627, in _getitem_axis\n",
      "    return self.obj._ixs(key, axis=axis)\n",
      "  File \"/home/alonsocampana/.local/lib/python3.9/site-packages/pandas/core/frame.py\", line 3723, in _ixs\n",
      "    result._set_is_copy(self, copy=copy)\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2024-09-25 17:08:09,714]\u001b[0m Trial 7 failed with value None.\u001b[0m\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if config[\"env\"][\"search_hyperparameters\"]:\n",
    "    study_name = f\"baseline_model\"\n",
    "    storage_name = \"sqlite:///studies/{}.db\".format(study_name)\n",
    "    study = optuna.create_study(study_name=study_name,\n",
    "                                storage=storage_name,\n",
    "                                direction='maximize',\n",
    "                                load_if_exists=True,\n",
    "                                pruner=optuna.pruners.MedianPruner(n_startup_trials=30,\n",
    "                                                               n_warmup_steps=5,\n",
    "                                                               interval_steps=5))\n",
    "    objective = lambda x: train_model_optuna(x, config)\n",
    "    study.optimize(objective, n_trials=40)\n",
    "    best_config = study.best_params\n",
    "    print(best_config)\n",
    "    config[\"model\"][\"embed_dim\"] = best_config[\"embed_dim\"]\n",
    "    config[\"model\"][\"hidden_dim\"] = best_config[\"hidden_dim\"]\n",
    "    config[\"model\"][\"n_layers\"] = best_config[\"n_layers\"]\n",
    "    config[\"model\"][\"norm\"] = best_config[\"norm\"]\n",
    "    config[\"model\"][\"dropout\"] = best_config[\"dropout\"]\n",
    "    config[\"optimizer\"][\"learning_rate\"] = best_config[\"learning_rate\"]\n",
    "    config[\"optimizer\"][\"clip_norm\"] = best_config[\"clip_norm\"]\n",
    "    config[\"optimizer\"][\"batch_size\"] = best_config[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620070ab-de87-486d-8a14-ca3791ff2803",
   "metadata": {},
   "source": [
    "# Model training and evaluation\n",
    "\n",
    "After we have a set of optimal hyperparameters we train our model. The train model function could be changed, but:\n",
    "- test_dataset cannot be used until we call the final evaluation step\n",
    "- the evaluation step cannot be modified, it must take the model produced by your pipeline, a dataloader that provides the correct data for your model, and the final metrics have to be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e603f6-c81a-4cb1-8c1b-d1d834a94169",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, model = scripts.train_model(config, torch.utils.data.ConcatDataset([train_dataset, validation_dataset]), None, use_momentum=False)\n",
    "device = torch.device(config[\"env\"][\"device\"])\n",
    "metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection(\n",
    "    {\"R_cellwise_residuals\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"drugs\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=True),\n",
    "    \"R_cellwise\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"cell_lines\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=False),\n",
    "    \"MSE\":torchmetrics.MeanSquaredError()}))\n",
    "metrics.to(device)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                       batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "                                       drop_last=False,\n",
    "                                      shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d785c4fb-4bd8-4bd2-b90b-81cb6e701cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSE': 1.870725154876709, 'R_cellwise': 0.886543869972229, 'R_cellwise_residuals': 0.29409846663475037}\n"
     ]
    }
   ],
   "source": [
    "final_metrics = scripts.evaluate_step(model, test_dataloader, metrics, device)\n",
    "print(final_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
